# System Architecture

## Enterprise RAG + Agentic AI with Next.js Frontend

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER BROWSER                             â”‚
â”‚                     http://localhost:3000                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NEXT.JS FRONTEND (CLIENT)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  UI Components:                                                  â”‚
â”‚  â€¢ ChatMessage.tsx    â†’ Display user/AI messages                â”‚
â”‚  â€¢ ChatInput.tsx      â†’ Input + file upload                     â”‚
â”‚  â€¢ SourceBadge.tsx    â†’ Show document sources                   â”‚
â”‚  â€¢ ConfidenceBadge.tsx â†’ Show confidence level                  â”‚
â”‚  â€¢ LoadingIndicator.tsx â†’ AI thinking animation                 â”‚
â”‚                                                                  â”‚
â”‚  Main Page:                                                      â”‚
â”‚  â€¢ app/page.tsx       â†’ Chat interface + state management       â”‚
â”‚                                                                  â”‚
â”‚  API Routes (Proxy):                                             â”‚
â”‚  â€¢ /api/chat          â†’ Forward to Python backend               â”‚
â”‚  â€¢ /api/upload        â†’ Forward file uploads to Python          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ HTTP Requests
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PYTHON BACKEND (BRAIN)                         â”‚
â”‚                  Flask API Server (Port 8000)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  API Endpoints:                                                  â”‚
â”‚  â€¢ POST /chat         â†’ Process user questions                  â”‚
â”‚  â€¢ POST /upload       â†’ Upload docs + re-index                  â”‚
â”‚  â€¢ GET /health        â†’ Health check                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      AGENTIC AI LAYER                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Intent Router:                                                  â”‚
â”‚  â€¢ Analyzes question intent                                     â”‚
â”‚  â€¢ Decides: RETRIEVE | REFUSE | ANSWER_DIRECTLY                 â”‚
â”‚                                                                  â”‚
â”‚  Answer Verifier:                                                â”‚
â”‚  â€¢ Validates answer against sources                             â”‚
â”‚  â€¢ Prevents hallucinations                                      â”‚
â”‚  â€¢ Ensures factual accuracy                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       RAG PIPELINE                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Document Ingestion:                                             â”‚
â”‚  â€¢ Load documents from data/raw/                                â”‚
â”‚  â€¢ Chunk into 1000 token segments                               â”‚
â”‚  â€¢ Generate embeddings (Google Gemini)                          â”‚
â”‚                                                                  â”‚
â”‚  Vector Store (ChromaDB):                                        â”‚
â”‚  â€¢ Store document embeddings                                    â”‚
â”‚  â€¢ Semantic similarity search                                   â”‚
â”‚  â€¢ Retrieve top-k relevant chunks                               â”‚
â”‚                                                                  â”‚
â”‚  QA Chain (LangChain):                                           â”‚
â”‚  â€¢ Combine query + retrieved docs                               â”‚
â”‚  â€¢ Generate answer via LLM                                      â”‚
â”‚  â€¢ Return answer + sources                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GOOGLE GEMINI API                             â”‚
â”‚                  (LLM + Embeddings)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Request Flow: User Question

```
1. User types question in ChatInput
   â†“
2. Frontend: POST /api/chat {"message": "..."}
   â†“
3. Next.js API Route: Proxy to backend
   â†“
4. Python Backend: Receive at /chat endpoint
   â†“
5. Intent Router: Analyze question intent
   â”œâ”€â”€ REFUSE â†’ Return "I don't know"
   â”œâ”€â”€ ANSWER_DIRECTLY â†’ Return conversational answer
   â””â”€â”€ RETRIEVE_AND_ANSWER â†’ Continue to RAG
       â†“
6. RAG Pipeline:
   â”œâ”€â”€ Query vectorstore for relevant docs
   â”œâ”€â”€ Retrieve top chunks
   â”œâ”€â”€ Send to LLM with context
   â””â”€â”€ Get answer
       â†“
7. Answer Verifier: Validate answer
   â”œâ”€â”€ Valid â†’ Return answer
   â””â”€â”€ Invalid â†’ Return "I don't know"
       â†“
8. Backend: Format response
   {
     "answer": "...",
     "sources": ["doc1.md"],
     "confidence": "High"
   }
   â†“
9. Frontend: Display in ChatMessage component
```

---

## Request Flow: Document Upload

```
1. User selects file in ChatInput
   â†“
2. Frontend: POST /api/upload (multipart/form-data)
   â†“
3. Next.js API Route: Forward file to backend
   â†“
4. Python Backend: Receive at /upload endpoint
   â†“
5. Save file to data/raw/
   â†“
6. Delete old vectorstore
   â†“
7. Re-ingest all documents
   â”œâ”€â”€ Load all files from data/raw/
   â”œâ”€â”€ Chunk documents
   â”œâ”€â”€ Generate embeddings
   â””â”€â”€ Create new vectorstore
       â†“
8. Re-initialize QA chain
   â†“
9. Backend: Return success
   {
     "success": true,
     "message": "Successfully uploaded and indexed"
   }
   â†“
10. Frontend: Show success notification
```

---

## Technology Stack

### Frontend
- **Framework**: Next.js 14 (React 18)
- **Language**: TypeScript
- **Styling**: Tailwind CSS
- **API**: Next.js API Routes (serverless functions)
- **State**: React Hooks (useState, useEffect)

### Backend
- **Framework**: Flask (Python web framework)
- **CORS**: Flask-CORS (cross-origin requests)
- **RAG**: LangChain (orchestration)
- **LLM**: Google Gemini Pro
- **Embeddings**: Google Gemini Embeddings
- **Vector DB**: ChromaDB
- **Agents**: Custom LangChain chains

---

## Data Flow

### Documents
```
User uploads .md/.pdf/.txt
    â†“
Saved to: data/raw/<filename>
    â†“
Ingested by: ingest/load_docs.py
    â†“
Chunked into: ~1000 token segments
    â†“
Embedded by: Google Gemini Embeddings
    â†“
Stored in: data/vectorstore/ (ChromaDB)
```

### Queries
```
User question
    â†“
Intent routing
    â†“
Query vectorstore (semantic search)
    â†“
Retrieve top 3 most similar chunks
    â†“
Combine with question â†’ prompt
    â†“
Send to Google Gemini Pro
    â†“
Generate answer
    â†“
Verify answer validity
    â†“
Return with sources + confidence
```

---

## Security & Separation of Concerns

### âœ… Frontend Responsibilities
- Display UI
- Handle user input
- Show loading states
- Display errors
- Upload files
- **NO BUSINESS LOGIC**

### âœ… Backend Responsibilities
- Intent routing
- Document retrieval
- Answer generation
- Answer verification
- Confidence calculation
- **ALL BUSINESS LOGIC**

### ğŸ”’ Security Notes
- API keys: Backend only (GOOGLE_API_KEY)
- LLM calls: Backend only
- Vector DB: Backend only
- CORS: Configured for localhost only
- File validation: Both frontend and backend
- No authentication (out of scope)

---

## Deployment Architecture

### Development
```
Frontend: localhost:3000 (npm run dev)
Backend:  localhost:8000 (python api_server.py)
```

### Production
```
Frontend: Vercel / AWS / Netlify
Backend:  Cloud Run / EC2 / Heroku
Vector DB: Persistent volume or managed service
```

---

## Key Design Decisions

1. **Frontend as Pure Client**
   - No LLM logic on frontend
   - All AI behavior controlled by backend
   - Frontend just displays results

2. **API Routes as Proxy**
   - Next.js API routes forward to Python
   - Keeps backend URL hidden from browser
   - Enables future authentication middleware

3. **Stateless Backend**
   - Each request is independent
   - No session management needed
   - Scales horizontally easily

4. **Agentic Workflow**
   - Intent routing before retrieval
   - Answer verification after generation
   - Confidence scoring for transparency

5. **Single-Page Application**
   - No routing complexity
   - All interactions in one view
   - Simple state management

---

## Performance Considerations

- **Frontend**: Static pages cached by Next.js
- **Backend**: QA chain initialized once on startup
- **Vector DB**: Persisted to disk, loaded once
- **LLM Calls**: Cached by LangChain (optional)
- **File Uploads**: Re-indexing blocks until complete

---

**This architecture ensures clean separation, scalability, and maintainability.**
